{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-04 17:59:31--  http://labfile.oss.aliyuncs.com/courses/722/tweets.json\n",
      "Resolving labfile.oss.aliyuncs.com (labfile.oss.aliyuncs.com)... 47.110.177.159\n",
      "Connecting to labfile.oss.aliyuncs.com (labfile.oss.aliyuncs.com)|47.110.177.159|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 331813 (324K) [application/json]\n",
      "Saving to: ‘tweets.json’\n",
      "\n",
      "tweets.json         100%[===================>] 324.04K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2019-03-04 17:59:31 (7.95 MB/s) - ‘tweets.json’ saved [331813/331813]\n",
      "\n",
      "--2019-03-04 17:59:31--  http://labfile.oss.aliyuncs.com/courses/722/donald.json\n",
      "Resolving labfile.oss.aliyuncs.com (labfile.oss.aliyuncs.com)... 47.110.177.159\n",
      "Connecting to labfile.oss.aliyuncs.com (labfile.oss.aliyuncs.com)|47.110.177.159|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26014 (25K) [application/json]\n",
      "Saving to: ‘donald.json’\n",
      "\n",
      "donald.json         100%[===================>]  25.40K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-03-04 17:59:31 (87.4 MB/s) - ‘donald.json’ saved [26014/26014]\n",
      "\n",
      "--2019-03-04 17:59:31--  http://labfile.oss.aliyuncs.com/courses/722/hillary.json\n",
      "Resolving labfile.oss.aliyuncs.com (labfile.oss.aliyuncs.com)... 47.110.177.159\n",
      "Connecting to labfile.oss.aliyuncs.com (labfile.oss.aliyuncs.com)|47.110.177.159|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 96311 (94K) [application/json]\n",
      "Saving to: ‘hillary.json’\n",
      "\n",
      "hillary.json        100%[===================>]  94.05K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2019-03-04 17:59:31 (11.5 MB/s) - ‘hillary.json’ saved [96311/96311]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget http://labfile.oss.aliyuncs.com/courses/722/tweets.json\n",
    "! wget http://labfile.oss.aliyuncs.com/courses/722/donald.json\n",
    "! wget http://labfile.oss.aliyuncs.com/courses/722/hillary.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-04 17:59:34--  http://labfile.oss.aliyuncs.com/courses/722/word2vecM_simple.zip\n",
      "Resolving labfile.oss.aliyuncs.com (labfile.oss.aliyuncs.com)... 47.110.177.159\n",
      "Connecting to labfile.oss.aliyuncs.com (labfile.oss.aliyuncs.com)|47.110.177.159|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3131111 (3.0M) [application/zip]\n",
      "Saving to: ‘word2vecM_simple.zip’\n",
      "\n",
      "word2vecM_simple.zi 100%[===================>]   2.99M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2019-03-04 17:59:34 (49.4 MB/s) - ‘word2vecM_simple.zip’ saved [3131111/3131111]\n",
      "\n",
      "Archive:  word2vecM_simple.zip\n",
      "   creating: word2vecM_simple/\n",
      "   creating: word2vecM_simple/data/\n",
      "  inflating: word2vecM_simple/data/.part-r-00000-5d5e245e-1d0c-4281-9274-145495a3565f.snappy.parquet.crc  \n",
      "  inflating: word2vecM_simple/data/._SUCCESS.crc  \n",
      "  inflating: word2vecM_simple/data/part-r-00000-5d5e245e-1d0c-4281-9274-145495a3565f.snappy.parquet  \n",
      "  inflating: word2vecM_simple/data/_SUCCESS  \n",
      "   creating: word2vecM_simple/metadata/\n",
      "  inflating: word2vecM_simple/metadata/.part-00000.crc  \n",
      "  inflating: word2vecM_simple/metadata/._SUCCESS.crc  \n",
      "  inflating: word2vecM_simple/metadata/part-00000  \n",
      "  inflating: word2vecM_simple/metadata/_SUCCESS  \n"
     ]
    }
   ],
   "source": [
    "!wget http://labfile.oss.aliyuncs.com/courses/722/word2vecM_simple.zip\n",
    "!unzip word2vecM_simple.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/data/spark-2.2.3-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding=utf8 -*- \n",
    "from __future__ import print_function\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark import SQLContext\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"sentiment_analysis\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#寻找推文的协调性\n",
    "#符号化推文的文本\n",
    "#删除停用词，标点符号，url等\n",
    "remove_spl_char_regex = re.compile('[%s]' % re.escape(string.punctuation))  # regex to remove special characters\n",
    "stopwords = [u'rt', u're', u'i', u'me', u'my', u'myself', u'we', u'our',               u'ours', u'ourselves', u'you', u'your',\n",
    "             u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers',\n",
    "             u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what',\n",
    "             u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were',\n",
    "             u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a',\n",
    "             u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by',\n",
    "             u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after',\n",
    "             u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under',\n",
    "             u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all',\n",
    "             u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not',\n",
    "             u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don',\n",
    "             u'should', u'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize函数对tweets内容进行分词\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "#     text = text.encode('ascii', 'ignore')  # to decode\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',\n",
    "                  text)  # to replace url with ''\n",
    "    text = remove_spl_char_regex.sub(\" \", text)  # Remove special characters\n",
    "    text = text.lower()\n",
    "\n",
    "    for word in text.split():\n",
    "        if word not in stopwords \\\n",
    "                and word not in string.punctuation \\\n",
    "                and len(word) > 1 \\\n",
    "                and word != '``':\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(document):\n",
    "    # 100维的向量\n",
    "    doc_vec = np.zeros(100)\n",
    "    tot_words = 0\n",
    "\n",
    "    for word in document:\n",
    "        try:\n",
    "        # 查找该词在预训练的word2vec模型中的特征值\n",
    "            vec = np.array(lookup_bd.value.get(word)) + 1\n",
    "            # print(vec)\n",
    "            # 若该特征词在预先训练好的模型中，则添加到向量中\n",
    "            if vec != None:\n",
    "                doc_vec += vec\n",
    "                tot_words += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    vec = doc_vec / float(tot_words)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- vector: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lookup = sqlContext.read.parquet(\"./word2vecM_simple/data\").alias(\"lookup\")\n",
    "lookup.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_bd = sc.broadcast(lookup.rdd.collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# 读入tweets.json作为分类器训练数据集\n",
    "with open('tweets.json', 'r') as f:\n",
    "    rawTrn_data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "trn_data = []\n",
    "for obj in rawTrn_data['results']:\n",
    "    token_text = tokenize(obj['text']) # 规范化推特文本，进行分词\n",
    "    tweet_text = doc2vec(token_text) # 将文本转换为向量\n",
    "    # 使用LabeledPoint 将文本对应的情感属性polariy：该条训练数据的标记label，tweet_text：训练分类器的features特征，结合成可作为spark mllib分类训练的数据类型\n",
    "    trn_data.append(LabeledPoint(obj['polarity'], tweet_text)) \n",
    "\n",
    "trnData = sc.parallelize(trn_data)\n",
    "#print(trnData)\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "# 读入hillary.json作为分类器测试数据集\n",
    "with open('hillary.json', 'r') as f:\n",
    "    rawTst_data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "tst_data = []\n",
    "for obj in rawTst_data['results']:\n",
    "    token_text = tokenize(obj['text'])\n",
    "    tweet_text = doc2vec(token_text)\n",
    "    tst_data.append(LabeledPoint(obj['polarity'], tweet_text))\n",
    "\n",
    "tst_dataRDD = sc.parallelize(tst_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_dataRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_dataRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    Predict: 1.0\n",
      "  Tree 1:\n",
      "    Predict: 1.0\n",
      "  Tree 2:\n",
      "    Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练随机森林分类器模型\n",
    "model = RandomForest.trainClassifier(trnData, numClasses=3, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "# 利用训练好的模型进行模型性能测试\n",
    "predictions = model.predict(tst_dataRDD.map(lambda x: x.features))\n",
    "labelsAndPredictions = tst_dataRDD.map(lambda lp: lp.label).zip(predictions)\n",
    "# 计算分类错误率\n",
    "# testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(tst_dataRDD.count())\n",
    "# print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')\n",
    "# 输出训练好的随机森林的分类决策模型\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
